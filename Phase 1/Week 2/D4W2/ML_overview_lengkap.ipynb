{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands On"
      ],
      "metadata": {
        "id": "lM2xGtyMbaUS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN6zZ_Ms5RkE"
      },
      "source": [
        "## 1 - Perkenalan\n",
        "\n",
        ">Bab pengenalan harus diisi dengan identitas, gambaran besar dataset yang digunakan, dan objective yang ingin dicapai.\n",
        "\n",
        "Nama:\n",
        "\n",
        "Batch:\n",
        "\n",
        "Problem Statement:  \n",
        "\n",
        "Objective:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZnEk82c5RkF"
      },
      "source": [
        "## 2 - Import Libraries\n",
        "\n",
        "> Cell pertama pada notebook harus berisi dan hanya berisi semua library yang digunakan dalam project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBTxcMAY5RkF"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "\n",
        "#import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#import FE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "\n",
        "#import model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "id": "XFyjKIUcIPcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFaSnACF5RkG"
      },
      "outputs": [],
      "source": [
        "!pip install feature_engine==1.6.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0zCH2z25RkG"
      },
      "source": [
        "## 3 - Data Loading\n",
        "\n",
        ">Bagian ini berisi proses penyiapan data sebelum dilakukan eksplorasi data lebih lanjut. Proses Data Loading dapat berupa memberi nama baru untuk setiap kolom, mengecek ukuran dataset, dll."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zhv8kJya5RkG"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/Vincentim27/data/refs/heads/main/beverages.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0jhOZUH5RkG"
      },
      "outputs": [],
      "source": [
        "#Duplicate Dataset\n",
        "\n",
        "data_duplicate = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mv5AEwt5RkG"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbO8mjuU5RkG"
      },
      "outputs": [],
      "source": [
        "#check dataset - 2\n",
        "\n",
        "data.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = df.columns.str.replace(' ', '_')\n",
        "df.columns"
      ],
      "metadata": {
        "id": "ae-i6EXMqC2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menambahkan 5 data baru yang berisi missing value\n",
        "new_data = {'Product_ID':[1001,1002,1003,1004,1005]\n",
        "            'Sales_Volume_(L)':2000,\n",
        "            'Product_Category':'Water',\n",
        "            'Price_per_Liter_(IDR)':5000,\n",
        "            'Advertising_Spend_(USD)':10000,\n",
        "            'Number_of_Retailers':[300,100,200,250,50],\n",
        "            'Temperature_(°C)':[22.2,18.0,21.5,18.7,24.3],\n",
        "            'Holiday_Season':[1,0,0,1,1],\n",
        "            'Market_Share_(%)':2.2,\n",
        "            'Competitor_Price_per_Liter_(IDR)':7000,\n",
        "            }\n",
        "\n",
        "df2 = df2._append(new_data, ignore_index=True)\n",
        "df2.tail()"
      ],
      "metadata": {
        "id": "MMpUSTNkoxgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdFnWInj5RkH"
      },
      "source": [
        "## 4 - Exploratory Data Analysis (EDA)\n",
        "\n",
        ">Bagian ini berisi eksplorasi data pada dataset diatas dengan menggunakan query, grouping, visualisasi sederhana, dan lain sebagainya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMuCHwdk5RkH"
      },
      "outputs": [],
      "source": [
        "# Create Histogram and Scatter plot\n",
        "\n",
        "plt.figure(figsize = (16,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.histplot(data['price'], kde = True, bins = 30)\n",
        "plt.title('Histogram of price')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.scatterplot(x = 'volume', y = 'margin', data= data)\n",
        "plt.title('volume Vs margin')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmnbCcRX5RkH"
      },
      "source": [
        "Statement ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PWq_CiG5RkH"
      },
      "source": [
        "## 5 - Feature Engineering\n",
        "\n",
        "> Bagian ini berisi proses penyiapan data untuk proses pelatihan model, seperti pembagian data menjadi train-test, transformasi data (normalisasi, encoding, dll.), dan proses-proses lain yang dibutuhkan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_3wG4W85RkH"
      },
      "source": [
        "#### Split between X (Features) and y (target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YICqvTLT5RkH"
      },
      "outputs": [],
      "source": [
        "#Splitting between 'X' and 'y'\n",
        "\n",
        "X = data.drop(['Holiday Season'], axis = 1)\n",
        "y = data['Holiday Season']\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnAceQd05RkH"
      },
      "source": [
        "### Splitting between Train-Set and Test-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipYmr6Mw5RkH"
      },
      "outputs": [],
      "source": [
        "#Splitting between train and test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\n",
        "print('Train Size: ', X_train.shape)\n",
        "print('Test Size: ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "fZH67r36-GlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_xV-S0J5RkI"
      },
      "source": [
        "### Handling Missing Value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtcdDJal5RkI"
      },
      "outputs": [],
      "source": [
        "X_train.isnull().sum().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOX4xxKB5RkI"
      },
      "outputs": [],
      "source": [
        "X_test.isnull().sum().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBT_4Jf35RkI"
      },
      "outputs": [],
      "source": [
        "y_train.isnull().sum().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHgEX9GA5RkI"
      },
      "outputs": [],
      "source": [
        "y_test.isnull().sum().mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing rows where is null\n",
        "null = df[df[''].isnull()]\n",
        "null"
      ],
      "metadata": {
        "id": "aATgqOCHvAD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check skewness\n",
        "X_train[''].skew()"
      ],
      "metadata": {
        "id": "XutIsfyAvlHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Define imputers\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Apply imputation to numeric columns\n",
        "X_train[['']] = median_imputer.fit_transform(X_train[['']])\n",
        "X_test[['']] = median_imputer.transform(X_test[['']])\n",
        "\n",
        "# Apply imputation to categorical columns\n",
        "X_train[['']] = mode_imputer.fit_transform(X_train[['']])\n",
        "X_test[['']] = mode_imputer.transform(X_test[['']])"
      ],
      "metadata": {
        "id": "ZBWB43GMvsX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.isnull().sum().mean()"
      ],
      "metadata": {
        "id": "mUTREvcbv4GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.isnull().sum().mean()"
      ],
      "metadata": {
        "id": "TQ5ETHRXv4_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.isnull().sum().mean()"
      ],
      "metadata": {
        "id": "ewbzrzBiv6K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.isnull().sum().mean()"
      ],
      "metadata": {
        "id": "iRZ7CXrpv7WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afcUgqwg5RkH"
      },
      "source": [
        "### Handling Outlier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to check skewness\n",
        "def check_skewness(df, *column_names):\n",
        "    return {col: df[col].skew() for col in column_names if col in df.columns}"
      ],
      "metadata": {
        "id": "My9UjTp6qnh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skewness_results = check_skewness(X_train, 'Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)',\n",
        "            'Number_of_Retailers','Temperature_(°C)','Market_Share_(%)','Competitor_Price_per_Liter_(IDR)')\n",
        "\n",
        "# Print skewness\n",
        "for col, skewness in skewness_results.items():\n",
        "    print(f\"{col}: {skewness}\")"
      ],
      "metadata": {
        "id": "qakglvQYqo0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists for each skewness category\n",
        "normal_columns = []\n",
        "skewed_columns = []\n",
        "extreme_skewed_columns = []\n",
        "\n",
        "# Loop through the skewness values and categorize the columns\n",
        "for col, skewness in skewness_results.items():\n",
        "    if skewness < -1.0 or skewness > 1.0:\n",
        "        extreme_skewed_columns.append(col)\n",
        "    elif abs(skewness) <= 0.5:  #or -> -0.5 <= skewness <= 0.5\n",
        "        normal_columns.append(col)\n",
        "    else:\n",
        "        skewed_columns.append(col)\n",
        "\n",
        "# Print the columns in each category\n",
        "print(f\"Normal: {normal_columns}\\nSkewed: {skewed_columns}\\nExtreme Skewed: {extreme_skewed_columns}\")"
      ],
      "metadata": {
        "id": "ay7LGz7Er26w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to calculate outlier percentages\n",
        "def calculate_outlier_percentages(df, columns, distance):\n",
        "    for variable in columns:\n",
        "        IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
        "        lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n",
        "        upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n",
        "\n",
        "        outliers = df[(df[variable] < lower_boundary) | (df[variable] > upper_boundary)]\n",
        "        outlier_percentage = len(outliers) / len(df) * 100\n",
        "\n",
        "        print('Percentage of outliers in {}: {:.2f}%'.format(variable, outlier_percentage))\n",
        "\n",
        "# Calcuate outlier percentages before handling\n",
        "calculate_outlier_percentages(X_train, skewed_columns, 1.5)\n",
        "calculate_outlier_percentages(X_train, extreme_skewed_columns, 3)"
      ],
      "metadata": {
        "id": "rlk-klSIr7Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure and two subplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Boxplot for skewed columns\n",
        "sns.boxplot(data=X_train[skewed_columns], orient=\"h\", ax=axes[0])\n",
        "axes[0].set_title(\"Boxplot for Skewed Columns\")\n",
        "axes[0].set_xlabel(\"Values\")\n",
        "axes[0].set_ylabel(\"Columns\")\n",
        "\n",
        "# Boxplot for extreme skewed columns\n",
        "sns.boxplot(data=X_train[extreme_skewed_columns], orient=\"h\", ax=axes[1])\n",
        "axes[1].set_title(\"Boxplot for Extreme Skewed Columns\")\n",
        "axes[1].set_xlabel(\"Values\")\n",
        "axes[1].set_ylabel(\"Columns\")\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JBH_0_7OsMPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to apply winsorization\n",
        "def apply_winsorization(train, variables, capping_method='iqr', tail='both', fold=3):\n",
        "    winsoriser = Winsorizer(capping_method=capping_method, tail=tail, fold=fold, variables=variables)\n",
        "    train_capped = winsoriser.fit_transform(train)\n",
        "    return train_capped\n",
        "\n",
        "# Apply to X_train column\n",
        "X_train = apply_winsorization(X_train, skewed_columns, fold=1.5)\n",
        "X_train = apply_winsorization(X_train, extreme_skewed_columns)"
      ],
      "metadata": {
        "id": "6V676_INsTmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the outliers after handling\n",
        "calculate_outlier_percentages(X_train, skewed_columns, 1.5)\n",
        "calculate_outlier_percentages(X_train, extreme_skewed_columns, 3)"
      ],
      "metadata": {
        "id": "emmSKHwNsiTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure and two subplots side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Boxplot for skewed columns\n",
        "sns.boxplot(data=X_train[skewed_columns], orient=\"h\", ax=axes[0])\n",
        "axes[0].set_title(\"Boxplot for Skewed Columns\")\n",
        "axes[0].set_xlabel(\"Values\")\n",
        "axes[0].set_ylabel(\"Columns\")\n",
        "\n",
        "# Boxplot for extreme skewed columns\n",
        "sns.boxplot(data=X_train[extreme_skewed_columns], orient=\"h\", whis=3, ax=axes[1])\n",
        "axes[1].set_title(\"Boxplot for Extreme Skewed Columns\")\n",
        "axes[1].set_xlabel(\"Values\")\n",
        "axes[1].set_ylabel(\"Columns\")\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mGt13Pqfsken"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk9URTES5RkI"
      },
      "source": [
        "### Feature Selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phik\n",
        "import phik"
      ],
      "metadata": {
        "id": "lSEuzOxuoY5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate X_train and y_train\n",
        "concat_train = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Show X_train\n",
        "concat_train.head()"
      ],
      "metadata": {
        "id": "BSNQiVmnsvlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to correlate variables with default\n",
        "def compute_phik_correlation(dataframe, columns):\n",
        "    subset = dataframe[columns]\n",
        "    correlation_matrix = subset.phik_matrix()\n",
        "    return correlation_matrix['default_payment_next_month']\n",
        "\n",
        "# Define the list of columns for each subset\n",
        "columns = ['Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)','Product_Category',\n",
        "            'Number_of_Retailers','Temperature_(°C)','Market_Share_(%)','Competitor_Price_per_Liter_(IDR)']\n",
        "\n",
        "# Compute Phi-K correlation for each set of columns and print\n",
        "correlation = compute_phik_correlation(concat_train, columns)\n",
        "\n",
        "# Print result\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "5vTlv6G9s9NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2aLNDyU5RkJ"
      },
      "outputs": [],
      "source": [
        "#Drop column that < 0.05\n",
        "X_train_cat.drop(['Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)'], axis = 1, inplace = True)\n",
        "X_test_cat.drop(['Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)'], axis = 1, inplace = True)\n",
        "X_train_cat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show columns\n",
        "print(X_train.columns)\n",
        "print(X_test.columns)"
      ],
      "metadata": {
        "id": "40ROLbVAtzE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split num col cat col"
      ],
      "metadata": {
        "id": "4z5OieM4uUxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_columns = X_train.select_dtypes(exclude=['object']).columns.tolist() # Cara1 - not recomended\n",
        "#cara 2:\n",
        "num_normal = []\n",
        "num_skew = []\n",
        "cat_encoded = []\n",
        "cat_ordinal = []\n",
        "cat_nominal = []\n",
        "print(f'Numerical normal columns:\\n{num_normal}')\n",
        "print(f'Numerical skew columns:\\n{num_skew}')\n",
        "print(f'Categorical encoded columns:\\n{cat_encoded}')"
      ],
      "metadata": {
        "id": "5K-SLSr3uQPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling using standard scaler\n",
        "scaler = StandardScaler()\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False) # sparse_output(atau sparse aja)=False utk mengganti -> `.toarray()`\n",
        "ord = OrdinalEncoder()\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers = [\n",
        "        ('num',scaler,num_columns),\n",
        "        ('nom',ohe,cat_columns),\n",
        "        ('ord',ord, cat_ordinal)],\n",
        "    remainder='passthrough' # untuk categorical yg sdh di encode\n",
        ")"
      ],
      "metadata": {
        "id": "M9Ir-uhHyk35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Modeling"
      ],
      "metadata": {
        "id": "b0RldQwZHPQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Membuat pipeline"
      ],
      "metadata": {
        "id": "scwvLcKrzgVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition using pipeline\n",
        "\n",
        "pipe_log = make_pipeline(preprocess,LogisticRegression(max_iter=1000000))\n",
        "pipe_svc = make_pipeline(preprocess,SVC())\n",
        "pipe_dt = make_pipeline(preprocess,DecisionTreeClassifier(random_state=10))\n",
        "pipe_rf = make_pipeline(preprocess,RandomForestClassifier(random_state=10))\n",
        "pipe_knn = make_pipeline(preprocess,KNeighborsClassifier())\n",
        "pipe_nb = make_pipeline(preprocess,GaussianNB())\n",
        "pipe_ada = make_pipeline(preprocess,AdaBoostClassifier())"
      ],
      "metadata": {
        "id": "yFI0e8Jcyq5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross validation untuk memilih best model"
      ],
      "metadata": {
        "id": "rfilF9ofzdBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting kfold\n",
        "skfold = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "# Define Cross Validation for each model\n",
        "cv_log_model = cross_val_score(pipe_log, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
        "cv_svm_model = cross_val_score(pipe_svc, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
        "cv_dt_model = cross_val_score(pipe_dt, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
        "cv_rf_model = cross_val_score(pipe_rf, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
        "cv_knn_model = cross_val_score(pipe_knn, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
        "cv_nb_model = cross_val_score(pipe_nb, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
        "cv_ada_model = cross_val_score(pipe_ada, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)"
      ],
      "metadata": {
        "id": "OvniRNsBytWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Best Model based on Cross_Val_Score (mean)\n",
        "name_model = []\n",
        "cv_scores = 0\n",
        "for cv,name in zip([cv_log_model,cv_svm_model,cv_dt_model,cv_rf_model,cv_knn_model,cv_nb_model,cv_ada_model],\n",
        "                   ['log_model','svm_model','dt_model','rf_model','knn_model','nb_model','ada_model']):\n",
        "  print(name)\n",
        "  print('f1score - All - Cross Validation :', cv)\n",
        "  print('f1score - Mean - Cross Validation :', cv.mean())\n",
        "  print('f1score - std - Cross Validation :', cv.std())\n",
        "  print('f1score - Range of Test Set :', (cv.mean()-cv.std()), '-' , (cv.mean()+cv.std()))\n",
        "  print('-'*50)\n",
        "  if cv.mean() > cv_scores:\n",
        "    cv_scores = cv.mean()\n",
        "    name_model = name\n",
        "  else:\n",
        "    pass\n",
        "print('Best Model:', name_model)\n",
        "print('Cross Val Mean from Best Model:', cv_scores)"
      ],
      "metadata": {
        "id": "zCfgKqCqy04Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit pipeline on the training data\n",
        "pipe_<best_model>.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2B_Fug3kzCYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions for both training and test data\n",
        "y_pred_train = pipe_<best_model>.predict(X_train)\n",
        "y_pred_test = pipe_<best_model>.predict(X_test)"
      ],
      "metadata": {
        "id": "ODcSgIs1zE0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation before tuning"
      ],
      "metadata": {
        "id": "g8ryj6GlzYVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print recall score\n",
        "print('Recall Score - Train Set  : ', recall_score(y_resample, y_pred_train))\n",
        "print('Recall Score - Test Set   : ', recall_score(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "c3OKQxfazUKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot confusion matrix for training data\n",
        "train_matrix = ConfusionMatrixDisplay.from_estimator(pipe_rf, X_resample, y_resample, cmap='PuBu', ax=axes[0])\n",
        "train_matrix.ax_.set_title('Confusion Matrix - Training Data')\n",
        "\n",
        "# Plot confusion matrix for test data\n",
        "test_matrix = ConfusionMatrixDisplay.from_estimator(pipe_rf, X_test, y_test, cmap='PuBu', ax=axes[1])\n",
        "test_matrix.ax_.set_title('Confusion Matrix - Test Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tdgq11zxzlMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simpan ke dalam table komparasi"
      ],
      "metadata": {
        "id": "eRi3tv4Dzphz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to create reports\n",
        "def performance_report(all_reports, y_resample, y_pred_train, y_test, y_pred_test, name):\n",
        "    # Calculate recall scores\n",
        "    score_reports = {\n",
        "        'Recall Train Set': recall_score(y_resample, y_pred_train),\n",
        "        'Recall Test Set': recall_score(y_test, y_pred_test),\n",
        "    }\n",
        "\n",
        "    # Calculate confusion matrices for train and test sets\n",
        "    cm_train = confusion_matrix(y_resample, y_pred_train)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    # Extract false negatives from the confusion matrices and add to the report\n",
        "    score_reports['False Negative Train'] = cm_train[1, 0]\n",
        "    score_reports['False Negative Test'] = cm_test[1, 0]\n",
        "\n",
        "    # Store the report in the dictionary with the specified model name\n",
        "    all_reports[name] = score_reports\n",
        "    return all_reports\n",
        "\n",
        "all_reports = {}\n",
        "all_reports = performance_report(all_reports, y_resample, y_pred_train, y_test, y_pred_test, 'Random Forest without Tuning')\n",
        "\n",
        "pd.DataFrame(all_reports)"
      ],
      "metadata": {
        "id": "CXpRiE7azo5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model tuning pakai `GridSearchCV`"
      ],
      "metadata": {
        "id": "MQ1aibSb0P4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# misal rf best model\n",
        "# Set up the parameter grid for Random Forest\n",
        "param_grid_rf = {\n",
        "    'classifier__n_estimators': [50, 100, 200, 300],\n",
        "    'classifier__max_depth': [1, 2, 3, 4],\n",
        "    'classifier__min_samples_split': [2, 3, 5, 7, 9],\n",
        "    'classifier__min_samples_leaf': [3, 5, 7, 9, 11],\n",
        "    'classifier__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV object for Random Forest\n",
        "grid_search_rf = RandomSearchCV(pipe_rf,\n",
        "                              param_grid=param_grid_rf,\n",
        "                              scoring='recall',\n",
        "                              cv=kf,\n",
        "                              verbose=2,\n",
        "                              n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV for Random Forest\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters for Random Forest\n",
        "print('Best hyperparameters for Random Forest:', grid_search_rf.best_params_)\n",
        "\n",
        "# Best recall for Random Forest\n",
        "print('Best recall for Random Forest:', grid_search_rf.best_score_)\n",
        "\n",
        "# Save best Random Forest model to a variable\n",
        "best_rf_model = grid_search_rf.best_estimator_"
      ],
      "metadata": {
        "id": "o9Esd2VXzxMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation after tuning"
      ],
      "metadata": {
        "id": "L1mmgz9V0Zm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions for training and testing set using the hyperparameter tuned model\n",
        "y_pred_train_tuned = best_rf_model.predict(X_train)\n",
        "y_pred_test_tuned = best_rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "NyronOVnz8Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print recall score\n",
        "print('Recall Score - Train Set  : ', recall_score(y_resample, y_pred_train_tuned))\n",
        "print('Recall Score - Test Set   : ', recall_score(y_test, y_pred_test_tuned))"
      ],
      "metadata": {
        "id": "7_fhTSkCz_IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot confusion matrix for training data\n",
        "train_matrix = ConfusionMatrixDisplay.from_estimator(best_rf_model, X_resample, y_resample, cmap='PuBu', ax=axes[0])\n",
        "train_matrix.ax_.set_title('Confusion Matrix - Training Data')\n",
        "\n",
        "# Plot confusion matrix for test data\n",
        "test_matrix = ConfusionMatrixDisplay.from_estimator(best_rf_model, X_test, y_test, cmap='PuBu', ax=axes[1])\n",
        "test_matrix.ax_.set_title('Confusion Matrix - Test Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FPaeT3zD0Bij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare before-after tuning"
      ],
      "metadata": {
        "id": "FKoGLzI_0dGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add results to the report\n",
        "all_reports = performance_report(all_reports, y_resample, y_pred_train_tuned, y_test, y_pred_test_tuned, 'Random Forest with Tuning')\n",
        "pd.DataFrame(all_reports)"
      ],
      "metadata": {
        "id": "9qZU4dsV0FMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model saving for Deployment"
      ],
      "metadata": {
        "id": "Hmd43p2a0LB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "with open('best_rf_model.pkl', 'wb') as model_file:\n",
        "  pickle.dump(best_rf_model, model_file)"
      ],
      "metadata": {
        "id": "0sHsKSMN0HJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "$$-- END --$$\n",
        "___"
      ],
      "metadata": {
        "id": "jQPNk9gajRlM"
      }
    }
  ]
}